
@article{ncbi_resource_coordinators_database_2018,
	title = {Database resources of the {National} {Center} for {Biotechnology} {Information}},
	volume = {46},
	issn = {0305-1048},
	url = {https://doi.org/10.1093/nar/gkx1095},
	doi = {10.1093/nar/gkx1095},
	abstract = {The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.},
	number = {D1},
	urldate = {2022-11-09},
	journal = {Nucleic Acids Research},
	author = {{NCBI Resource Coordinators}},
	month = jan,
	year = {2018},
	pages = {D8--D13},
}

@techreport{molder_sustainable_2021,
	title = {Sustainable data analysis with {Snakemake}},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	url = {https://f1000research.com/articles/10-33},
	abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way.\&nbsp;Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid. Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
	language = {en},
	number = {10:33},
	urldate = {2022-11-09},
	institution = {F1000Research},
	author = {Mölder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and Tomkins-Tinch, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and Köster, Johannes},
	month = apr,
	year = {2021},
	doi = {10.12688/f1000research.29032.2},
	note = {Type: article},
	keywords = {adaptability, data analysis, reproducibility, scalability, snakemake, sustainability, transparency, workflow management},
}

@misc{fernandez-guerra_genomewalkeramgsim-smk_2022,
	title = {genomewalker/{aMGSIM}-smk: v0.0.1},
	url = {https://doi.org/10.5281/zenodo.7298422},
	publisher = {Zenodo},
	author = {Fernandez-Guerra, Antonio},
	month = nov,
	year = {2022},
	doi = {10.5281/zenodo.7298422},
}

@article{parks_standardized_2018,
	title = {A standardized bacterial taxonomy based on genome phylogeny substantially revises the tree of life},
	volume = {36},
	copyright = {2018 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.4229},
	doi = {10.1038/nbt.4229},
	abstract = {Interpretation of microbial genome data will be improved by a fully revised bacterial taxonomy.},
	language = {en},
	number = {10},
	urldate = {2022-11-09},
	journal = {Nature Biotechnology},
	author = {Parks, Donovan H. and Chuvochina, Maria and Waite, David W. and Rinke, Christian and Skarshewski, Adam and Chaumeil, Pierre-Alain and Hugenholtz, Philip},
	month = nov,
	year = {2018},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Bacteria, Phylogenetics, Taxonomy},
	pages = {996--1004},
}

@article{nayfach_checkv_2021,
	title = {{CheckV} assesses the quality and completeness of metagenome-assembled viral genomes},
	volume = {39},
	copyright = {2020 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-020-00774-7},
	doi = {10.1038/s41587-020-00774-7},
	abstract = {Millions of new viral sequences have been identified from metagenomes, but the quality and completeness of these sequences vary considerably. Here we present CheckV, an automated pipeline for identifying closed viral genomes, estimating the completeness of genome fragments and removing flanking host regions from integrated proviruses. CheckV estimates completeness by comparing sequences with a large database of complete viral genomes, including 76,262 identified from a systematic search of publicly available metagenomes, metatranscriptomes and metaviromes. After validation on mock datasets and comparison to existing methods, we applied CheckV to large and diverse collections of metagenome-assembled viral sequences, including IMG/VR and the Global Ocean Virome. This revealed 44,652 high-quality viral genomes (that is, {\textgreater}90\% complete), although the vast majority of sequences were small fragments, which highlights the challenge of assembling viral genomes from short-read metagenomes. Additionally, we found that removal of host contamination substantially improved the accurate identification of auxiliary metabolic genes and interpretation of viral-encoded functions.},
	language = {en},
	number = {5},
	urldate = {2022-11-09},
	journal = {Nature Biotechnology},
	author = {Nayfach, Stephen and Camargo, Antonio Pedro and Schulz, Frederik and Eloe-Fadrosh, Emiley and Roux, Simon and Kyrpides, Nikos C.},
	month = may,
	year = {2021},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {CheckV, Genome informatics, Metagenomics},
	pages = {578--585},
}

@article{cabanski_reqon_2012,
	title = {{ReQON}: a {Bioconductor} package for recalibrating quality scores from next-generation sequencing data},
	volume = {13},
	issn = {1471-2105},
	shorttitle = {{ReQON}},
	url = {https://doi.org/10.1186/1471-2105-13-221},
	doi = {10.1186/1471-2105-13-221},
	abstract = {Next-generation sequencing technologies have become important tools for genome-wide studies. However, the quality scores that are assigned to each base have been shown to be inaccurate. If the quality scores are used in downstream analyses, these inaccuracies can have a significant impact on the results.},
	number = {1},
	urldate = {2022-10-31},
	journal = {BMC Bioinformatics},
	author = {Cabanski, Christopher R. and Cavin, Keary and Bizon, Chris and Wilkerson, Matthew D. and Parker, Joel S. and Wilhelmsen, Kirk C. and Perou, Charles M. and Marron, JS and Hayes, D. Neil},
	month = sep,
	year = {2012},
	keywords = {Bioconductor, Bioinformatics, Next-generation sequencing, Quality score, Recalibration, reqon},
	pages = {221},
}

@article{wang_ngslcatoolkit_nodate,
	title = {{ngsLCA}—{A} toolkit for fast and flexible lowest common ancestor inference and taxonomic profiling of metagenomic data},
	volume = {n/a},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.14006},
	doi = {10.1111/2041-210X.14006},
	abstract = {Metagenomic data generated from environmental samples is increasingly common in the analysis of modern and ancient biological communities. To obtain taxonomic profiles from this type of data, DNA sequences are aligned against large genomic reference databases and the lowest common ancestor (LCA) needs to be inferred for each sequence with multiple alignments. To date, efforts have mainly focused on improving the speed, sensitivity and specificity of alignment tools, and little effort has been applied to the LCA algorithm that generates the taxonomic profiles from alignments. We present ngsLCA, a command-line toolkit with two separate modules: the main program (in C/C++) performing LCA inference, and an R package for generating tables and visualisations of the taxonomic profiles. ngsLCA processed large datasets in BAM/SAM alignment format 4–11 times faster and used less memory compared to other available programs. It is compatible with the NCBI taxonomy and has flexible parameter settings. Furthermore, the toolkit offers functions for filtering, contamination removal, taxonomic clustering, and multiple ways of visualising the generated taxonomic profiles. ngsLCA bridges a gap in current metagenomic analyses by supplying a computationally light, easy-to-use, accurate, fast and flexible LCA algorithm with R functions for processing and illustrating the taxonomic profiles},
	language = {en},
	number = {n/a},
	urldate = {2022-10-31},
	journal = {Methods in Ecology and Evolution},
	author = {Wang, Yucheng and Korneliussen, Thorfinn Sand and Holman, Luke E. and Manica, Andrea and Pedersen, Mikkel Winther},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.14006},
	keywords = {environmental DNA (eDNA), lowest common ancestor (LCA), metagenomics, next-generation sequencing, ngslca, sedimentary ancient DNA (sedaDNA), shotgun sequencing, taxonomic profiling, toolkit},
}

@misc{michelsen_pydamage_2022,
	title = {{PyDamage}},
	copyright = {GPL-3.0},
	url = {https://github.com/ChristianMichelsen/pydamage-decimals},
	abstract = {Damage parameter estimation for ancient DNA},
	urldate = {2022-10-31},
	author = {Michelsen, Christian},
	month = oct,
	year = {2022},
	note = {original-date: 2022-10-28T14:17:52Z},
	keywords = {pydamage-decimals},
}

@misc{plotly,
	title = {Collaborative data science},
	url = {https://plot.ly},
	author = {Plotly, Technologies},
	year = {2015},
	note = {Place: Montreal, QC
Publisher: Plotly Technologies Inc.},
	keywords = {plotly},
}

@article{langmead_fast_2012,
	title = {Fast gapped-read alignment with {Bowtie} 2},
	volume = {9},
	copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1923},
	doi = {10.1038/nmeth.1923},
	abstract = {The Bowtie 2 software achieves fast, sensitive, accurate and memory-efficient gapped alignment of sequencing reads using the full-text minute index and hardware-accelerated dynamic programming algorithms.},
	language = {en},
	number = {4},
	urldate = {2022-10-20},
	journal = {Nature Methods},
	author = {Langmead, Ben and Salzberg, Steven L.},
	month = apr,
	year = {2012},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Genomics, Sequencing, bowtie, bowtie2},
	pages = {357--359},
}

@article{huang_art_2012,
	title = {{ART}: a next-generation sequencing read simulator},
	volume = {28},
	issn = {1367-4811},
	shorttitle = {{ART}},
	doi = {10.1093/bioinformatics/btr708},
	abstract = {ART is a set of simulation tools that generate synthetic next-generation sequencing reads. This functionality is essential for testing and benchmarking tools for next-generation sequencing data analysis including read alignment, de novo assembly and genetic variation discovery. ART generates simulated sequencing reads by emulating the sequencing process with built-in, technology-specific read error models and base quality value profiles parameterized empirically in large sequencing datasets. We currently support all three major commercial next-generation sequencing platforms: Roche's 454, Illumina's Solexa and Applied Biosystems' SOLiD. ART also allows the flexibility to use customized read error model parameters and quality profiles.
AVAILABILITY: Both source and binary software packages are available at http://www.niehs.nih.gov/research/resources/software/art.},
	language = {eng},
	number = {4},
	journal = {Bioinformatics (Oxford, England)},
	author = {Huang, Weichun and Li, Leping and Myers, Jason R. and Marth, Gabor T.},
	month = feb,
	year = {2012},
	pmid = {22199392},
	pmcid = {PMC3278762},
	keywords = {Chromosomes, Human, Pair 17, Genetic Variation, High-Throughput Nucleotide Sequencing, Humans, Sequence Analysis, DNA, Software, art},
	pages = {593--594},
}

@article{skoglund_separating_2014,
	title = {Separating endogenous ancient {DNA} from modern day contamination in a {Siberian} {Neandertal}},
	volume = {111},
	url = {https://www.pnas.org/doi/10.1073/pnas.1318934111},
	doi = {10.1073/pnas.1318934111},
	number = {6},
	urldate = {2022-10-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Skoglund, Pontus and Northoff, Bernd H. and Shunkov, Michael V. and Derevianko, Anatoli P. and Pääbo, Svante and Krause, Johannes and Jakobsson, Mattias},
	month = feb,
	year = {2014},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {PMD},
	pages = {2229--2234},
}

@book{gelman_bayesian_2015,
	address = {New York},
	edition = {3},
	title = {Bayesian {Data} {Analysis}},
	isbn = {978-0-429-11307-9},
	abstract = {Winner of the 2016 De Groot Prize from the International Society for Bayesian AnalysisNow in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied},
	publisher = {Chapman and Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	month = jul,
	year = {2015},
	doi = {10.1201/b16018},
}

@article{ginolhac_mapdamage_2011,
	title = {{mapDamage}: testing for damage patterns in ancient {DNA} sequences},
	volume = {27},
	issn = {1367-4803},
	shorttitle = {{mapDamage}},
	url = {https://doi.org/10.1093/bioinformatics/btr347},
	doi = {10.1093/bioinformatics/btr347},
	abstract = {Summary: Ancient DNA extracts consist of a mixture of contaminant DNA molecules, most often originating from environmental microbes, and endogenous fragments exhibiting substantial levels of DNA damage. The latter introduce specific nucleotide misincorporations and DNA fragmentation signatures in sequencing reads that could be advantageously used to argue for sequence validity. mapDamage is a Perl script that computes nucleotide misincorporation and fragmentation patterns using next-generation sequencing reads mapped against a reference genome. The Perl script outputs are further automatically processed in embedded R script in order to detect typical patterns of genuine ancient DNA sequences.Availability and implementation: The Perl script mapDamage is freely available with documentation and example files at http://geogenetics.ku.dk/all\_literature/mapdamage/. The script requires prior installation of the SAMtools suite and R environment and has been validated on both GNU/Linux and MacOSX operating systems.Contact:aginolhac@snm.ku.dkSupplementary information:Supplementary data available at Bioinformatics online},
	number = {15},
	urldate = {2022-03-29},
	journal = {Bioinformatics},
	author = {Ginolhac, Aurelien and Rasmussen, Morten and Gilbert, M. Thomas P. and Willerslev, Eske and Orlando, Ludovic},
	month = aug,
	year = {2011},
	pages = {2153--2155},
}

@article{llamas_field_2017,
	title = {From the field to the laboratory: {Controlling} {DNA} contamination in human ancient {DNA} research in the high-throughput sequencing era},
	volume = {3},
	issn = {2054-8923},
	shorttitle = {From the field to the laboratory},
	url = {https://www.tandfonline.com/doi/full/10.1080/20548923.2016.1258824},
	doi = {10.1080/20548923.2016.1258824},
	abstract = {High-Throughput DNA Sequencing (HTS) technologies have changed the way in which we detect and assess DNA contamination in ancient DNA studies. Researchers use computational methods to mine the large quantity of sequencing data to detect characteristic patterns of DNA damage, and to evaluate the authenticity of the results. We argue that unless computational methods can conﬁdently separate authentic ancient DNA sequences from contaminating DNA that displays damage patterns under independent decay processes, prevention and control of DNA contamination should remain a central and critical aspect of ancient human DNA studies. Ideally, DNA contamination can be prevented early on by following minimal guidelines during excavation, sample collection and/or subsequent handling. Contaminating DNA should also be monitored or minimised in the ancient DNA laboratory using specialised facilities and strict experimental procedures. In this paper, we update recommendations to control for DNA contamination from the ﬁeld to the laboratory, in an attempt to facilitate communication between ﬁeld archaeologists, anthropologists and ancient DNA researchers. We also provide updated criteria of ancient DNA authenticity for HTS-based studies. We are conﬁdent that the procedures outlined here will increase the retrieval of higher proportions of authentic genetic information from valuable archaeological human remains in the future.},
	language = {en},
	number = {1},
	urldate = {2022-03-29},
	journal = {STAR: Science \& Technology of Archaeological Research},
	author = {Llamas, Bastien and Valverde, Guido and Fehren-Schmitz, Lars and Weyrich, Laura S and Cooper, Alan and Haak, Wolfgang},
	month = jan,
	year = {2017},
	pages = {1--14},
}

@article{borry_pydamage_2021,
	title = {{PyDamage}: automated ancient damage identification and estimation for contigs in ancient {DNA} de novo assembly},
	volume = {9},
	issn = {2167-8359},
	shorttitle = {{PyDamage}},
	url = {https://peerj.com/articles/11845},
	doi = {10.7717/peerj.11845},
	abstract = {DNA de novo assembly can be used to reconstruct longer stretches of DNA (contigs), including genes and even genomes, from short DNA sequencing reads. Applying this technique to metagenomic data derived from archaeological remains, such as paleofeces and dental calculus, we can investigate past microbiome functional diversity that may be absent or underrepresented in the modern microbiome gene catalogue. However, compared to modern samples, ancient samples are often burdened with environmental contamination, resulting in metagenomic datasets that represent mixtures of ancient and modern DNA. The ability to rapidly and reliably establish the authenticity and integrity of ancient samples is essential for ancient DNA studies, and the ability to distinguish between ancient and modern sequences is particularly important for ancient microbiome studies. Characteristic patterns of ancient DNA damage, namely DNA fragmentation and cytosine deamination (observed as C-to-T transitions) are typically used to authenticate ancient samples and sequences, but existing tools for inspecting and filtering aDNA damage either compute it at the read level, which leads to high data loss and lower quality when used in combination with de novo assembly, or require manual inspection, which is impractical for ancient assemblies that typically contain tens to hundreds of thousands of contigs. To address these challenges, we designed PyDamage, a robust, automated approach for aDNA damage estimation and authentication of de novo assembled aDNA. PyDamage uses a likelihood ratio based approach to discriminate between truly ancient contigs and contigs originating from modern contamination. We test PyDamage on both on simulated aDNA data and archaeological paleofeces, and we demonstrate its ability to reliably and automatically identify contigs bearing DNA damage characteristic of aDNA. Coupled with aDNA de novo assembly, Pydamage opens up new doors to explore functional diversity in ancient metagenomic datasets.},
	language = {en},
	urldate = {2022-03-29},
	journal = {PeerJ},
	author = {Borry, Maxime and Hübner, Alexander and Rohrlach, Adam B. and Warinner, Christina},
	month = jul,
	year = {2021},
	note = {Publisher: PeerJ Inc.},
	pages = {e11845},
}

@article{dabney_ancient_2013,
	title = {Ancient {DNA} {Damage}},
	volume = {5},
	issn = {1943-0264},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3685887/},
	doi = {10.1101/cshperspect.a012567},
	abstract = {Under favorable conditions DNA can survive for thousands of years in the remains of dead organisms. The DNA extracted from such remains is invariably degraded to a small average size by processes that at least partly involve depurination. It also contains large amounts of deaminated cytosine residues that are accumulated toward the ends of the molecules, as well as several other lesions that are less well characterized., DNA fragments from ancient specimens are short (40–500 bp) and contain lesions that block DNA polymerases and cause replication errors. Degradation involves depurination and cytosine deamination, but other processes may be involved.},
	number = {7},
	urldate = {2022-03-28},
	journal = {Cold Spring Harbor Perspectives in Biology},
	author = {Dabney, Jesse and Meyer, Matthias and Pääbo, Svante},
	month = jul,
	year = {2013},
	pmid = {23729639},
	pmcid = {PMC3685887},
	keywords = {mikkel},
	pages = {a012567},
}

@article{cappellini_ancient_2018,
	title = {Ancient {Biomolecules} and {Evolutionary} {Inference}},
	volume = {87},
	url = {https://doi.org/10.1146/annurev-biochem-062917-012002},
	doi = {10.1146/annurev-biochem-062917-012002},
	abstract = {Over the past three decades, studies of ancient biomolecules—particularly ancient DNA, proteins, and lipids—have revolutionized our understanding of evolutionary history. Though initially fraught with many challenges, today the field stands on firm foundations. Researchers now successfully retrieve nucleotide and amino acid sequences, as well as lipid signatures, from progressively older samples, originating from geographic areas and depositional environments that, until recently, were regarded as hostile to long-term preservation of biomolecules. Sampling frequencies and the spatial and temporal scope of studies have also increased markedly, and with them the size and quality of the data sets generated. This progress has been made possible by continuous technical innovations in analytical methods, enhanced criteria for the selection of ancient samples, integrated experimental methods, and advanced computational approaches. Here, we discuss the history and current state of ancient biomolecule research, its applications to evolutionary inference, and future directions for this young and exciting field.},
	number = {1},
	urldate = {2022-03-28},
	journal = {Annual Review of Biochemistry},
	author = {Cappellini, Enrico and Prohaska, Ana and Racimo, Fernando and Welker, Frido and Pedersen, Mikkel Winther and Allentoft, Morten E. and de Barros Damgaard, Peter and Gutenbrunner, Petra and Dunne, Julie and Hammann, Simon and Roffet-Salque, Mélanie and Ilardo, Melissa and Moreno-Mayar, J. Víctor and Wang, Yucheng and Sikora, Martin and Vinner, Lasse and Cox, Jürgen and Evershed, Richard P. and Willerslev, Eske},
	year = {2018},
	pmid = {29709200},
	note = {\_eprint: https://doi.org/10.1146/annurev-biochem-062917-012002},
	keywords = {ancient DNA, ancient genomics, ancient lipids, ancient proteins, mikkel, paleogenomics, paleoproteomics},
	pages = {1029--1060},
}

@article{bezanson2017julia,
	title = {Julia: {A} fresh approach to numerical computing},
	volume = {59},
	url = {https://julialang.org/},
	number = {1},
	journal = {SIAM review},
	author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
	year = {2017},
	note = {Publisher: SIAM},
	keywords = {julia},
	pages = {65--98},
}

@article{gelman_understanding_2014,
	title = {Understanding predictive information criteria for {Bayesian} models},
	volume = {24},
	issn = {0960-3174, 1573-1375},
	doi = {10.1007/s11222-013-9416-2},
	abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
	language = {en},
	number = {6},
	journal = {Statistics and Computing},
	author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
	month = nov,
	year = {2014},
	keywords = {waic},
	pages = {997--1016},
}

@article{hoffman_no-u-turn_2011,
	title = {The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}},
	shorttitle = {The {No}-{U}-{Turn} {Sampler}},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
	journal = {arXiv:1111.4246 [cs, stat]},
	author = {Hoffman, Matthew D. and Gelman, Andrew},
	month = nov,
	year = {2011},
	note = {arXiv: 1111.4246},
	keywords = {Computer Science - Machine Learning, NUTS, Statistics - Computation},
}

@misc{dembinski_scikit-hepiminuit_2021,
	title = {scikit-hep/iminuit: v2.8.2},
	copyright = {Open Access},
	shorttitle = {scikit-hep/iminuit},
	abstract = {See changelog on RTD},
	urldate = {2021-10-05},
	publisher = {Zenodo},
	author = {Dembinski, Hans and Piti Ongmongkolkul and Deil, Christoph and Hurtado, David Menéndez and Schreiner, Henry and Feickert, Matthew and Andrew and Burr, Chris and Watson, Jason and Rost, Fabian and Pearce, Alex and Geiger, Lukas and Wiedemann, Bernhard M. and Gohlke, Christoph and Gonzalo and Drotleff, Jonas and Eschle, Jonas and Neste, Ludwig and Gorelli, Marco Edward and Baak, Max and Zapata, Omar and Odidev},
	month = aug,
	year = {2021},
	doi = {10.5281/ZENODO.3949207},
	keywords = {iminuit},
}

@inproceedings{lam_numba_2015,
	address = {New York, NY, USA},
	series = {{LLVM} '15},
	title = {Numba: a {LLVM}-based {Python} {JIT} compiler},
	isbn = {978-1-4503-4005-2},
	shorttitle = {Numba},
	url = {https://github.com/numba/numba},
	doi = {10.1145/2833157.2833162},
	abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
	booktitle = {Proceedings of the {Second} {Workshop} on the {LLVM} {Compiler} {Infrastructure} in {HPC}},
	publisher = {Association for Computing Machinery},
	author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
	month = nov,
	year = {2015},
	keywords = {LLVM, Python, compiler, numba},
	pages = {1--6},
}

@article{cepeda-cuervo_double_2017,
	title = {Double {Generalized} {Beta}-{Binomial} and {Negative} {Binomial} {Regression} {Models}},
	volume = {40},
	issn = {0120-1751},
	doi = {10.15446/rce.v40n1.61779},
	language = {en},
	number = {1},
	journal = {Revista Colombiana de Estadística},
	author = {Cepeda-Cuervo, Edilberto and Cifuentes-Amado, MARíA VICTORIA},
	month = jan,
	year = {2017},
	note = {Publisher: Universidad Nacional de Colombia.},
	pages = {141--163},
}

@article{phan_composable_2019,
	title = {Composable {Effects} for {Flexible} and {Accelerated} {Probabilistic} {Programming} in {NumPyro}},
	abstract = {NumPyro is a lightweight library that provides an alternate NumPy backend to the Pyro probabilistic programming language with the same modeling interface, language primitives and effect handling abstractions. Effect handlers allow Pyro's modeling API to be extended to NumPyro despite its being built atop a fundamentally different JAX-based functional backend. In this work, we demonstrate the power of composing Pyro's effect handlers with the program transformations that enable hardware acceleration, automatic differentiation, and vectorization in JAX. In particular, NumPyro provides an iterative formulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT compiled, yielding an implementation that is much faster than existing alternatives in both the small and large dataset regimes.},
	journal = {arXiv:1912.11554 [cs, stat]},
	author = {Phan, Du and Pradhan, Neeraj and Jankowiak, Martin},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.11554},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, G.3, I.2.5, I.2.5, G.3, Statistics - Machine Learning, numpyro},
}

@article{watanabe_asymptotic_2010,
	title = {Asymptotic {Equivalence} of {Bayes} {Cross} {Validation} and {Widely} {Applicable} {Information} {Criterion} in {Singular} {Learning} {Theory}},
	volume = {11},
	issn = {1533-7928},
	abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.},
	number = {116},
	journal = {Journal of Machine Learning Research},
	author = {Watanabe, Sumio},
	year = {2010},
	keywords = {waic},
	pages = {3571--3594},
}

@article{betancourt_conceptual_2018,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	journal = {arXiv:1701.02434 [stat]},
	author = {Betancourt, Michael},
	month = jul,
	year = {2018},
	note = {arXiv: 1701.02434},
	keywords = {HMC, Statistics - Methodology},
}

@article{briggs_patterns_2007,
	title = {Patterns of damage in genomic {DNA} sequences from a {Neandertal}},
	volume = {104},
	copyright = {© 2007 by The National Academy of Sciences of the USA},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/104/37/14616},
	doi = {10.1073/pnas.0704665104},
	abstract = {High-throughput direct sequencing techniques have recently opened the possibility to sequence genomes from Pleistocene organisms. Here we analyze DNA sequences determined from a Neandertal, a mammoth, and a cave bear. We show that purines are overrepresented at positions adjacent to the breaks in the ancient DNA, suggesting that depurination has contributed to its degradation. We furthermore show that substitutions resulting from miscoding cytosine residues are vastly overrepresented in the DNA sequences and drastically clustered in the ends of the molecules, whereas other substitutions are rare. We present a model where the observed substitution patterns are used to estimate the rate of deamination of cytosine residues in single- and double-stranded portions of the DNA, the length of single-stranded ends, and the frequency of nicks. The results suggest that reliable genome sequences can be obtained from Pleistocene organisms.},
	language = {en},
	number = {37},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Briggs, Adrian W. and Stenzel, Udo and Johnson, Philip L. F. and Green, Richard E. and Kelso, Janet and Prüfer, Kay and Meyer, Matthias and Krause, Johannes and Ronan, Michael T. and Lachmann, Michael and Pääbo, Svante},
	month = sep,
	year = {2007},
	pmid = {17715061},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {454, deamination, depurination, paleogenomics},
	pages = {14616--14621},
}

@book{mcelreath_statistical_2020,
	address = {Boca Raton},
	edition = {2},
	series = {{CRC} texts in statistical science},
	title = {Statistical rethinking: a {Bayesian} course with examples in {R} and {Stan}},
	isbn = {978-0-367-13991-9},
	shorttitle = {Statistical rethinking},
	abstract = {"Statistical Rethinking: A Bayesian Course with Examples in R and Stan, Second Edition builds knowledge/confidence in statistical modeling. Pushes readers to perform step-by-step calculations (usually automated.) Unique, computational approach ensures readers understand details to make reasonable choices and interpretations in their modeling work"--},
	publisher = {Taylor and Francis, CRC Press},
	author = {McElreath, Richard},
	year = {2020},
}

@article{jonsson_mapdamage20_2013,
	title = {{mapDamage2}.0: fast approximate {Bayesian} estimates of ancient {DNA} damage parameters},
	volume = {29},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{mapDamage2}.0},
	doi = {10.1093/bioinformatics/btt193},
	abstract = {Motivation: Ancient DNA (aDNA) molecules in fossilized bones and teeth, coprolites, sediments, mummified specimens and museum collections represent fantastic sources of information for evolutionary biologists, revealing the agents of past epidemics and the dynamics of past populations. However, the analysis of aDNA generally faces two major issues. Firstly, sequences consist of a mixture of endogenous and various exogenous backgrounds, mostly microbial. Secondly, high nucleotide misincorporation rates can be observed as a result of severe post-mortem DNA damage. Such misincorporation patterns are instrumental to authenticate ancient sequences versus modern contaminants. We recently developed the user-friendly mapDamage package that identifies such patterns from next-generation sequencing (NGS) sequence datasets. The absence of formal statistical modeling of the DNA damage process, however, precluded rigorous quantitative comparisons across samples.},
	language = {en},
	number = {13},
	journal = {Bioinformatics},
	author = {Jónsson, Hákon and Ginolhac, Aurélien and Schubert, Mikkel and Johnson, Philip L. F. and Orlando, Ludovic},
	month = jul,
	year = {2013},
	keywords = {mapDamage},
	pages = {1682--1684},
}

@misc{bradbury_jax_2018,
	title = {{JAX}: composable transformations of {Python} {NumPy} programs},
	url = {http://github.com/google/jax},
	author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and Vander\{P\}las, Jake and Wanderman-\{M\}ilne, Skye and Zhang, Qiao},
	year = {2018},
}

@article{neukamm_damageprofiler_nodate,
	title = {{DamageProfiler}: {Fast} damage pattern calculation for ancient {DNA}},
	abstract = {In ancient DNA research, the authentication of ancient samples based on specific features remains a crucial step in data analysis. Because of this central importance, researchers lacking deeper programming knowledge should be able to run a basic damage authentication analysis. Such software should be user-friendly and easy to integrate into an analysis pipeline. Here, we present DamageProfiler, a Java based, stand-alone software to determine damage patterns in ancient DNA. The results are provided in various file formats and plots for further processing. DamageProfiler has an intuitive graphical as well as command line interface that allows the tool to be easily embedded into an analysis pipeline.},
	language = {en},
	author = {Neukamm, Judith and Peltzer, Alexander and Nieselt, Kay},
	keywords = {damageprofiler},
	pages = {10},
}
